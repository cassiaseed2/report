import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import time
import csv
from datetime import datetime
from copy import deepcopy
import numpy as np
from collections import defaultdict
import joblib
from functools import partial
import os
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import traceback
import warnings

# 忽略特定警告
warnings.filterwarnings("ignore", category=UserWarning)

def load_mnist_data(batch_size=64, augment=True, data_dir='./mnist_data'):
    """加载MNIST数据集并进行预处理"""
    os.makedirs(data_dir, exist_ok=True)
    
    base_transforms = [
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ]
    
    augment_transforms = []
    if augment:
        augment_transforms = [
            transforms.RandomRotation(10),
            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),
            transforms.ColorJitter(brightness=0.2, contrast=0.2)
        ]
    
    train_transforms = transforms.Compose(augment_transforms + base_transforms)
    test_transforms = transforms.Compose(base_transforms)
    
    required_files = [
        'train-images-idx3-ubyte',
        'train-labels-idx1-ubyte',
        't10k-images-idx3-ubyte',
        't10k-labels-idx1-ubyte'
    ]
    
    files_exist = all(os.path.exists(os.path.join(data_dir, f)) for f in required_files)
    
    try:
        train_set = torchvision.datasets.MNIST(
            root=data_dir,
            train=True,
            download=not files_exist,
            transform=train_transforms
        )
        test_set = torchvision.datasets.MNIST(
            root=data_dir,
            train=False,
            download=not files_exist,
            transform=test_transforms
        )
    except Exception as e:
        print(f"加载数据集时出错: {e}")
        print("尝试使用PyTorch默认下载源...")
        train_set = torchvision.datasets.MNIST(
            root=data_dir,
            train=True,
            download=True,
            transform=train_transforms
        )
        test_set = torchvision.datasets.MNIST(
            root=data_dir,
            train=False,
            download=True,
            transform=test_transforms
        )
    
    train_loader = DataLoader(
        train_set,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0,  # 设置为0避免多进程问题
        pin_memory=True
    )
    test_loader = DataLoader(
        test_set,
        batch_size=batch_size,
        shuffle=False,
        num_workers=0,  # 设置为0避免多进程问题
        pin_memory=True
    )
    
    print(f"训练集大小: {len(train_set)} 样本")
    print(f"测试集大小: {len(test_set)} 样本")
    return train_loader, test_loader

# 定义统一的 CNN 模型
class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc1 = nn.Linear(64*7*7, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = self.pool(x)
        x = F.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(-1, 64*7*7)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class AdvancedTrainer:
    def __init__(self, device="cpu", log_dir="logs", num_workers=0):  # 默认num_workers=0
        self.device = device
        self.log_dir = log_dir
        self.num_workers = num_workers
        os.makedirs(log_dir, exist_ok=True)
    
    def train_and_evaluate(self, model, train_loader, val_loader, criterion, optimizer, 
                         scheduler=None, epochs=10, model_name="model",
                         patience=3, checkpoint_freq=5, grad_clip=1.0):
        """高级训练与评估系统"""
        writer = SummaryWriter(os.path.join(self.log_dir, model_name))
        history = defaultdict(list)
        
        best_val_acc = 0.0
        best_val_loss = float('inf')
        best_model_weights = None
        epochs_no_improve = 0
        
        model.to(self.device)
        
        # 只在 CUDA 设备上启用混合精度
        use_amp = self.device.type == 'cuda'
        scaler = torch.cuda.amp.GradScaler() if use_amp else None
        
        # 修复梯度累积步骤计算
        accum_steps = max(1, 64 // train_loader.batch_size)
        if accum_steps == 0:
            accum_steps = 1
        
        for epoch in range(epochs):
            epoch_start = time.time()
            model.train()
            train_loss, train_correct, train_total = 0.0, 0, 0
            optimizer.zero_grad()
            
            for batch_idx, (images, labels) in enumerate(train_loader):
                images, labels = images.to(self.device), labels.to(self.device)
                
                # 混合精度训练
                if use_amp:
                    with torch.cuda.amp.autocast():
                        outputs = model(images)
                        loss = criterion(outputs, labels) / accum_steps
                else:
                    outputs = model(images)
                    loss = criterion(outputs, labels) / accum_steps
                
                if scaler:
                    scaler.scale(loss).backward()
                else:
                    loss.backward()
                
                # 梯度累积
                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1) == len(train_loader):
                    if scaler:
                        scaler.unscale_(optimizer)
                        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
                        scaler.step(optimizer)
                        scaler.update()
                    else:
                        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)
                        optimizer.step()
                    optimizer.zero_grad()
                
                # 统计指标
                train_loss += loss.item() * images.size(0) * accum_steps
                _, predicted = torch.max(outputs.data, 1)
                train_total += labels.size(0)
                train_correct += (predicted == labels).sum().item()
            
            # 验证阶段
            val_loss, val_correct, val_total = self.evaluate_model(model, val_loader, criterion)
            
            # 计算指标
            train_loss = train_loss / train_total
            train_acc = 100 * train_correct / train_total
            val_loss = val_loss / val_total
            val_acc = 100 * val_correct / val_total
            
            # 更新学习率
            current_lr = optimizer.param_groups[0]['lr']
            
            if scheduler:
                if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):
                    scheduler.step(val_loss)
                else:
                    scheduler.step()
            
            # 记录历史
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            history['epoch_time'].append(time.time() - epoch_start)
            history['learning_rate'].append(current_lr)
            
            # TensorBoard记录
            writer.add_scalar('Loss/train', train_loss, epoch)
            writer.add_scalar('Loss/val', val_loss, epoch)
            writer.add_scalar('Accuracy/train', train_acc, epoch)
            writer.add_scalar('Accuracy/val', val_acc, epoch)
            writer.add_scalar('Learning Rate', current_lr, epoch)
            
            # 改进的早停机制
            improvement = False
            if val_acc > best_val_acc + 0.001:
                best_val_acc = val_acc
                improvement = True
            
            if val_loss < best_val_loss - 0.001:
                best_val_loss = val_loss
                improvement = True
            
            if improvement:
                best_model_weights = deepcopy(model.state_dict())
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': best_model_weights,
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                    'val_acc': val_acc,
                    'val_loss': val_loss
                }, os.path.join(self.log_dir, f'best_{model_name}.pth'))
                epochs_no_improve = 0
            else:
                epochs_no_improve += 1
                if epochs_no_improve >= patience:
                    print(f"Early stopping at epoch {epoch+1}")
                    break
            
            # 定期保存检查点
            if (epoch + 1) % checkpoint_freq == 0:
                torch.save({
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
                }, os.path.join(self.log_dir, f'checkpoint_{model_name}_epoch{epoch+1}.pth'))
            
            # 打印进度 - 修复了这里的语法错误
            print(f"Epoch [{epoch+1}/{epochs}] "
                  f"Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | "
                  f"Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}% | "
                  f"LR: {current_lr:.2e} | Time: {history['epoch_time'][-1]:.2f}s")
        
        writer.close()
        
        # 恢复最佳权重
        if best_model_weights:
            model.load_state_dict(best_model_weights)
            
        return dict(history), best_model_weights

    def evaluate_model(self, model, data_loader, criterion):
        """优化的评估函数"""
        model.eval()
        total_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in data_loader:
                images, labels = images.to(self.device), labels.to(self.device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                total_loss += loss.item() * images.size(0)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        
        return total_loss, correct, total

    def hyperparameter_search(self, model_class, train_dataset, val_dataset, param_grid, 
                            epochs=10, n_trials=20, search_strategy='random'):
        """高级超参数搜索"""
        results = []
        cache_file = os.path.join(self.log_dir, "hyperparam_cache.pkl")
        
        if os.path.exists(cache_file):
            try:
                cached_results = joblib.load(cache_file)
                results.extend(cached_results)
                print(f"Loaded {len(cached_results)} cached results")
            except:
                pass
        
        param_combinations = self._generate_param_combinations(param_grid, n_trials, search_strategy)
        print(f"Starting hyperparameter search with {len(param_combinations)} combinations...")
        
        # 修复：使用列表推导确保所有参数是基本类型
        param_combinations = [
            {
                'lr': float(params['lr']),
                'batch_size': int(params['batch_size']),  # 转换为Python int
                'weight_decay': float(params['weight_decay']),
                'scheduler': params.get('scheduler', None)
            }
            for params in param_combinations
        ]
        
        # 使用单进程避免环境问题
        new_results = []
        for params in param_combinations:
            result = self._train_single_config(params, model_class, train_dataset, val_dataset, epochs)
            new_results.append(result)
        
        results.extend(new_results)
        self._save_results_to_csv(results)
        joblib.dump(results, cache_file)
        
        return results

    def _train_single_config(self, params, model_class, train_dataset, val_dataset, epochs):
        """训练单个超参数配置"""
        try:
            print(f"\n=== Testing combination: {params} ===")
            
            # 确保batch_size是整数
            batch_size = int(params['batch_size'])
            
            # 使用单进程数据加载器
            train_loader = DataLoader(
                train_dataset, 
                batch_size=batch_size,
                shuffle=True,
                num_workers=0,  # 使用0避免多进程问题
                pin_memory=True
            )
            val_loader = DataLoader(
                val_dataset,
                batch_size=batch_size,
                shuffle=False,
                num_workers=0,  # 使用0避免多进程问题
                pin_memory=True
            )
            
            model = model_class().to(self.device)
            optimizer = optim.Adam(
                model.parameters(), 
                lr=params['lr'],
                weight_decay=params['weight_decay']
            )
            
            scheduler = None
            # 修复：移除 verbose 参数
            if params.get('scheduler') == 'plateau':
                scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer, mode='min', factor=0.5, patience=2)
            elif params.get('scheduler') == 'cosine':
                scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)
            
            # 修复了这里的字符串格式化语法错误
            model_name = f"model_lr{params['lr']:.0e}_bs{params['batch_size']}_wd{params['weight_decay']:.0e}"
            if 'scheduler' in params:
                model_name += f"_sch{params['scheduler']}"
            
            history, _ = self.train_and_evaluate(
                model, train_loader, val_loader, 
                nn.CrossEntropyLoss(), optimizer,
                scheduler=scheduler,
                epochs=epochs,
                model_name=model_name,
                patience=3
            )
            
            result = {
                'lr': params['lr'],
                'batch_size': params['batch_size'],
                'weight_decay': params['weight_decay'],
                'scheduler': params.get('scheduler', None),
                'best_val_acc': max(history['val_acc']),
                'final_train_acc': history['train_acc'][-1],
                'total_time': sum(history['epoch_time']),
                'model_name': model_name,
                'epochs_trained': len(history['train_acc']),
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            }
            
            return result
        except Exception as e:
            print(f"Error in training config {params}: {str(e)}")
            traceback.print_exc()
            return None

    def _generate_param_combinations(self, param_grid, n_trials, strategy='random'):
        """生成超参数组合"""
        if strategy == 'grid':
            from itertools import product
            keys = param_grid.keys()
            values = [param_grid[key] for key in keys]
            return [dict(zip(keys, combo)) for combo in product(*values)]
        
        param_combinations = []
        for _ in range(n_trials):
            params = {}
            for key, values in param_grid.items():
                if isinstance(values, list):
                    params[key] = np.random.choice(values)
                elif callable(values):
                    params[key] = values()
                else:
                    if key == 'lr':
                        params[key] = 10**np.random.uniform(-5, -1)
                    elif key == 'weight_decay':
                        params[key] = 10**np.random.uniform(-6, -2)
                    elif key == 'batch_size':
                        # 确保batch_size是整数
                        params[key] = int(2**np.random.randint(5, 10))
            param_combinations.append(params)
        
        return param_combinations

    def _save_results_to_csv(self, results, filename="hyperparam_results.csv"):
        """保存结果到CSV文件"""
        if not results:
            return
            
        filepath = os.path.join(self.log_dir, filename)
        file_exists = os.path.isfile(filepath)
        
        # 过滤掉None结果
        valid_results = [r for r in results if r is not None]
        if not valid_results:
            return
        
        # 获取所有可能的键作为表头
        all_keys = set()
        for result in valid_results:
            all_keys.update(result.keys())
        headers = sorted(all_keys)
        
        with open(filepath, 'a', newline='') as f:
            writer = csv.writer(f)
            
            if not file_exists:
                writer.writerow(headers)
            
            for result in valid_results:
                row = [result.get(k, '') for k in headers]
                writer.writerow(row)

    def find_best_model(self, results):
        """从结果中找到最佳模型"""
        if not results:
            return None, None
        
        valid_results = [r for r in results if r is not None]
        if not valid_results:
            return None, None
            
        best_result = max(valid_results, key=lambda x: x['best_val_acc'])
        
        model_path = os.path.join(self.log_dir, f"best_{best_result['model_name']}.pth")
        if os.path.exists(model_path):
            return model_path, best_result
        
        return None, best_result

# 使用示例
if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Using device: {device}")
    
    # 创建高级训练器
    trainer = AdvancedTrainer(device=device, log_dir="experiments", num_workers=0)
    
    # 加载真实MNIST数据
    train_loader, test_loader = load_mnist_data(batch_size=64, augment=True)
    train_dataset = train_loader.dataset
    test_dataset = test_loader.dataset
    
    # 定义参数搜索空间
    param_grid = {
        'lr': [0.01, 0.001, 0.0001],
        'batch_size': [64, 128, 256],
        'weight_decay': [0, 1e-4],
        'scheduler': ['plateau', 'cosine', None]
    }
    
    # 执行超参数搜索
    search_results = trainer.hyperparameter_search(
        CNN,
        train_dataset,
        test_dataset,  # 使用测试集作为验证集
        param_grid,
        epochs=5,  # 减少epochs以快速测试
        n_trials=3,  # 减少试验次数
        search_strategy='random'
    )
    
    # 找到并加载最佳模型
    if search_results:
        best_model_path, best_result = trainer.find_best_model(search_results)
        if best_model_path:
            print(f"\nBest model: {best_model_path}")
            print(f"Validation accuracy: {best_result['best_val_acc']:.2f}%")
            
            # 加载最佳模型
            model = CNN().to(device)
            checkpoint = torch.load(best_model_path)
            model.load_state_dict(checkpoint['model_state_dict'])
            
            # 评估最佳模型
            total = 0
            correct = 0
            with torch.no_grad():
                for images, labels in test_loader:
                    images, labels = images.to(device), labels.to(device)
                    outputs = model(images)
                    _, predicted = torch.max(outputs.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()
            
            accuracy = 100 * correct / total
            print(f"Test Accuracy: {accuracy:.2f}%")
        else:
            print("No best model found")
    else:
        print("Hyperparameter search failed")
